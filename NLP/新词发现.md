###### 最新成果

https://spaces.ac.cn/archives/6920

###### 基于信息熵

1. 主要考虑频数、凝固度、自由度
2. https://spaces.ac.cn/archives/3491 （代码实现）
3. http://www.matrix67.com/blog/archives/5044

###### 基于切分

https://spaces.ac.cn/archives/3913/comment-page-1#comments

1. 重点是去除了边界熵
2. 可以得到任意长度词语

![image-20210626111852563](/Users/djc/Library/Application Support/typora-user-images/image-20210626111852563.png)

```python
import pymongo

db = pymongo.MongoClient().baike.items
def texts():
    for a in db.find(no_cursor_timeout=True).limit(1000000):
        yield a['content']

from collections import defaultdict #defaultdict是经过封装的dict，它能够让我们设定默认值
from tqdm import tqdm #tqdm是一个非常易用的用来显示进度的库
from math import log
import re

class Find_Words:
    def __init__(self, min_count=10, min_pmi=0):
        self.min_count = min_count
        self.min_pmi = min_pmi
        self.chars, self.pairs = defaultdict(int), defaultdict(int) #如果键不存在，那么就用int函数
                                                                  #初始化一个值，int()的默认结果为0
        self.total = 0.
    def text_filter(self, texts): #预切断句子，以免得到太多无意义（不是中文、英文、数字）的字符串
        for a in tqdm(texts):
            for t in re.split(u'[^\u4e00-\u9fa50-9a-zA-Z]+', a): #这个正则表达式匹配的是任意非中文、
                                                              #非英文、非数字，因此它的意思就是用任
                                                              #意非中文、非英文、非数字的字符断开句子
                if t:
                    yield t
    def count(self, texts): #计数函数，计算单字出现频数、相邻两字出现的频数
        for text in self.text_filter(texts):
            self.chars[text[0]] += 1
            for i in range(len(text)-1):
                self.chars[text[i+1]] += 1
                self.pairs[text[i:i+2]] += 1
                self.total += 1
        self.chars = {i:j for i,j in self.chars.items() if j >= self.min_count} #最少频数过滤
        self.pairs = {i:j for i,j in self.pairs.items() if j >= self.min_count} #最少频数过滤
        self.strong_segments = set()
        for i,j in self.pairs.items(): #根据互信息找出比较“密切”的邻字
            _ = log(self.total*j/(self.chars[i[0]]*self.chars[i[1]]))
            if _ >= self.min_pmi:
                self.strong_segments.add(i)
    def find_words(self, texts): #根据前述结果来找词语
        self.words = defaultdict(int)
        for text in self.text_filter(texts):
            s = text[0]
            for i in range(len(text)-1):
                if text[i:i+2] in self.strong_segments: #如果比较“密切”则不断开
                    s += text[i+1]
                else:
                    self.words[s] += 1 #否则断开，前述片段作为一个词来统计
                    s = text[i+1]
            self.words[s] += 1 #最后一个“词”
        self.words = {i:j for i,j in self.words.items() if j >= self.min_count} #最后再次根据频数过滤

fw = Find_Words(16, 1)
fw.count(texts())
fw.find_words(texts())

import pandas as pd
words = pd.Series(fw.words).sort_values(ascending=False)
```

Python流式读取SQL数据的参考代码：

```python
from sqlalchemy import *

def sql_data_generator():
    db = create_engine('mysql+pymysql://user:password@123.456.789.123/yourdatabase?charset=utf8')
    result = db.execution_options(stream_results=True).execute(text('select content from articles'))
    for t in result:
        yield t[0]
```

###### 更好的算法

https://spaces.ac.cn/archives/4256，

既然分词是为了削弱相关性，那么我们分词，就是在相关性弱的地方切断了

完整的算法步骤如下：

> 第一步，统计：选取某个固定的nn，统计2grams、3grams、…、ngrams，计算它们的内部凝固度，只保留高于某个阈值的片段，构成一个集合GG；这一步，可以为2grams、3grams、…、ngrams设置不同的阈值，不一定要相同，因为字数越大，一般来说统计就越不充分，越有可能偏高，所以字数越大，阈值要越高；
>
> 第二步，切分：用上述grams对语料进行切分（粗糙的分词），并统计频率。切分的规则是，只要一个片段出现在前一步得到的集合GG中，这个片段就不切分，比如“各项目”，只要“各项”和“项目”都在GG中，这时候就算“各项目”不在GG中，那么“各项目”还是不切分，保留下来；
>
> 第三步，回溯：经过第二步，“各项目”会被切出来（因为第二步保证宁放过，不切错）。回溯就是检查，如果它是一个小于等于nn字的词，那么检测它在不在GG中，不在就出局；如果它是一个大于nn字的词，那个检测它每个nn字片段是不是在GG中，只要有一个片段不在，就出局。还是以“各项目”为例，回溯就是看看，“各项目”在不在3gram中，不在的话，就得出局。

每一步的补充说明：

> 1、使用较高的凝固度，但综合考虑多字，是为了更准，比如两字的“共和”不会出现在高凝固度集合中，所以会切开（比如“我一共和三个人去玩”，“共和”就切开了），但三字“共和国”出现在高凝固度集合中，所以“中华人民共和国”的“共和”不会切开；
>
> 2、第二步就是根据第一步筛选出来的集合，对句子进行切分（你可以理解为粗糙的分词），然后把“粗糙的分词结果”做统计，注意现在是统计分词结果，跟第一步的凝固度集合筛选没有交集，我们认为虽然这样的分词比较粗糙，但高频的部分还是靠谱的，所以筛选出高频部分；
>
> 3、第三步，例如因为“各项”和“项目”都出现高凝固度的片段中，所以第二步我们也不会把“各项目”切开，但我们不希望“各项目”成词，因为“各”跟“项目”的凝固度不高（“各”跟“项”的凝固度高，不代表“各”跟“项目”的凝固度高），所以通过回溯，把“各项目”移除（只需要看一下“各项目”在不在原来统计的高凝固度集合中即可，所以这步计算量是很小的）

## 代码实现[ #](https://spaces.ac.cn/archives/4256/comment-page-1#代码实现)

下面给出一个参考的代码实现。首先，为了节约内存，写一个迭代器来逐篇输出文章：

```python
import re
import pymongo
from tqdm import tqdm
import hashlib

db = pymongo.MongoClient().weixin.text_articles
md5 = lambda s: hashlib.md5(s).hexdigest()

def texts():
    texts_set = set()
    for a in tqdm(db.find(no_cursor_timeout=True).limit(3000000)):
        if md5(a['text'].encode('utf-8')) in texts_set:
            continue
        else:
            texts_set.add(md5(a['text'].encode('utf-8')))
            for t in re.split(u'[^\u4e00-\u9fa50-9a-zA-Z]+', a['text']):
                if t:
                    yield t
    print u'最终计算了%s篇文章' % len(texts_set)
```

需要解释的是：我的文章存在mongodb中，所以用pymongo读，如果文章存在文件中，做法是类似的。当然，内存足够，而文章不多的话，直接把文章以列表的形式加载到内存中也没有问题；引入hashlib是为了对文章去重；引入正则表达式re是为了预先去掉无意义字符（非中文、非英文、非数字）；引入tqdm是为了显示进度。

接着，直接计数：

```python
from collections import defaultdict
import numpy as np

n = 4
min_count = 128
ngrams = defaultdict(int)

for t in texts():
    for i in range(len(t)):
        for j in range(1, n+1):
            if i+j <= len(t):
                ngrams[t[i:i+j]] += 1

ngrams = {i:j for i,j in ngrams.iteritems() if j >= min_count}
total = 1.*sum([j for i,j in ngrams.iteritems() if len(i) == 1])
```

这里的nn就是需要考虑的最长片段的字数（前面说的ngrams），建议至少设为3，min_count看需求而设。接着就是凝固度的筛选了：

```python
min_proba = {2:5, 3:25, 4:125}

def is_keep(s, min_proba):
    if len(s) >= 2:
        score = min([total*ngrams[s]/(ngrams[s[:i+1]]*ngrams[s[i+1:]]) for i in range(len(s)-1)])
        if score > min_proba[len(s)]:
            return True
    else:
        return False

ngrams_ = set(i for i,j in ngrams.iteritems() if is_keep(i, min_proba))
```

前文已经说了，可以为不同长度的gram设置不同的阈值，因此用了个字典来制定阈值。个人感觉，阈值成5倍的等比数列比较好，当然，这个还有看数据大小。接着，定义切分函数，并进行切分统计：

```python
def cut(s):
    r = np.array([0]*(len(s)-1))
    for i in range(len(s)-1):
        for j in range(2, n+1):
            if s[i:i+j] in ngrams_:
                r[i:i+j-1] += 1
    w = [s[0]]
    for i in range(1, len(s)):
        if r[i-1] > 0:
            w[-1] += s[i]
        else:
            w.append(s[i])
    return w

words = defaultdict(int)
for t in texts():
    for i in cut(t):
        words[i] += 1

words = {i:j for i,j in words.iteritems() if j >= min_count}
```

最后，回溯：

```python
def is_real(s):
    if len(s) >= 3:
        for i in range(3, n+1):
            for j in range(len(s)-i+1):
                if s[j:j+i] not in ngrams_:
                    return False
        return True
    else:
        return True

w = {i:j for i,j in words.iteritems() if is_real(i)}
```

构建词库

https://kexue.fm/archives/5597