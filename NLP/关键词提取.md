1. word2vec提取

https://spaces.ac.cn/archives/4316

![image-20210626154852661](/Users/djc/Library/Application Support/typora-user-images/image-20210626154852661.png)

这时候读者应该会明白，为什么我在前两篇文章中会那么强调Skip-Gram + Huffman Softmax这个组合了，因为这个组合就是对p(wk|wi)p(wk|wi)进行建模的。当然，由于Huffman Softmax的特性，我们要算p(wk|wi)p(wk|wi)，需要费一些周折，参考代码如下：

```python
import numpy as np
import gensim
model = gensim.models.word2vec.Word2Vec.load('word2vec_wx')

def predict_proba(oword, iword):
    iword_vec = model[iword]
    oword = model.wv.vocab[oword]
    oword_l = model.syn1[oword.point].T
    dot = np.dot(iword_vec, oword_l)
    lprob = -sum(np.logaddexp(0, -dot) + oword.code*dot) 
    return lprob
```

有了上面的铺垫，现在算关键词就简单了：

```python
from collections import Counter
def keywords(s):
    s = [w for w in s if w in model]
    ws = {w:sum([predict_proba(u, w) for u in s]) for w in s}
    return Counter(ws).most_common()

import pandas as pd #引入它主要是为了更好的显示效果
import jieba
s = u'太阳是一颗恒星'
pd.Series(keywords(jieba.cut(s)))
```